<base target="_blank">
<H1>ANYU Dataset for Monocular Depth Estimation</H1>

D. Ignatov, A. Ignatov, R. Timofte. <B>Virtually Enriched NYU Depth V2 Dataset for Monocular Depth Estimation: Do We Need Artificial Augmentation?</B> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2024.

Aritfacts:
  
- virtually augmented NYU Depth V2 training dataset (<a href="https://drive.google.com/file/d/1nrsiowQW1L9IEYLWoiGfJAhD56nSA3Sx/view?usp=sharing" target="_blank">ANYU</a>) extended with 10% and 100% artificially modified images; 
- <a href="https://drive.google.com/file/d/1Uwhv50z1ke13O0X34WFsRTKaph8rqKqs/view?usp=sharing" target="_blank">virtually enriched (100 %) NYU Depth V2 testing set</a>: 2048 artificially modified RGB-D testing pairs of images; 
- <a href="https://github.com/ABrain-One/ANYU/blob/main/README.md" target="_blank">the VPD model checkpoint</a>.

The dataset contains RGB-D images from NYU depth v2. When using the ANYU dataset, this work should be cited along with our paper:
N. Silberman, D. Hoiem, P. Kohli, R. Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012.
