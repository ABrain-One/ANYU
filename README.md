<base target="_blank">
<H1>Artifacts of the article</H1>
<center><H2>D. Ignatov, A. Ignatov, R. Timofte. <br/> <B>Virtually Enriched NYU Depth V2 Dataset for Monocular Depth Estimation:<br/> Do We Need Artificial Augmentation?</B> <br/> In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2024.</H2></center>center>

- virtually augmented NYU Depth V2 training dataset (<a href="https://drive.google.com/file/d/1nrsiowQW1L9IEYLWoiGfJAhD56nSA3Sx/view?usp=sharing" target="_blank">ANYU</a>) extended with 10% and 100% artificially modified images;
- <a href="https://drive.google.com/file/d/1Uwhv50z1ke13O0X34WFsRTKaph8rqKqs/view?usp=sharing" target="_blank">virtually enriched (100 %) NYU Depth V2 testing set</a>: 2048 artificially modified RGB-D testing pairs of images; 
- <a href="https://drive.google.com/file/d/1xl3_CwEmPWtbswuS5ZW8K-ad8Xi7GRMr/view?usp=sharing" target="_blank">the VPD model checkpoint</a>.

The dataset contains RGB-D images from NYU depth v2. When using the ANYU dataset, this work should be cited along with our paper:
N. Silberman, D. Hoiem, P. Kohli, R. Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012.
