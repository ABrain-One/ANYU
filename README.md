<base target="_blank">
<H1>ANYU Dataset for Monocular Depth Estimation</H1>

Aritfacts:
  
- virtually augmented NYU Depth V2 training dataset with adding of <a href="https://drive.google.com/file/d/1saf-S8PXGKd01j0wfKnndSdNMIXpsTc2/view?usp=sharing" target="_blank">10% and 100%</a> of artificially modified images; 
- <a href="https://drive.google.com/file/d/1Uwhv50z1ke13O0X34WFsRTKaph8rqKqs/view?usp=sharing" target="_blank">virtually enriched (100 %) NYU Depth V2 testing set</a>: 2048 artificially modified RGB-D testing pairs of images; 
- <a href="https://github.com/ABrain-One/ANYU/blob/main/README.md" target="_blank">the VPD model checkpoint</a>.

The dataset contains RGB-D images from NYU depth v2 [2]. When using the ANYU dataset, this work should be cited along with our paper:
1. D. Ignatov, A. Ignatov, R. Timofte. Virtually Enriched NYU Depth V2 Dataset for Monocular Depth Estimation: Do We Need Artificial Augmentation? In CVPR, 2024.
2. N. Silberman, D. Hoiem, P. Kohli, R. Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012.
