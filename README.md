<base target="_blank">
## <B>Virtually Enriched NYU Depth V2 Dataset for Monocular Depth Estimation:<br/> Do We Need Artificial Augmentation?</B> 
## D. Ignatov, A. Ignatov, R. Timofte. 
## In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2024.
Artifacts of the article:
- virtually augmented NYU Depth V2 training dataset (<a href="https://drive.google.com/file/d/1nrsiowQW1L9IEYLWoiGfJAhD56nSA3Sx/view?usp=sharing" target="_blank">ANYU</a>) extended with 10% and 100% artificially modified images;
- <a href="https://drive.google.com/file/d/14FXyJHCUAxIxtbwlY5R4GkfOZp3_CeYm/view?usp=sharing" target="_blank">virtually enriched (100 %) NYU Depth V2 testing set</a>: 2048 artificially modified RGB-D testing pairs of images; 
- <a href="https://drive.google.com/file/d/1xl3_CwEmPWtbswuS5ZW8K-ad8Xi7GRMr/view?usp=sharing" target="_blank">the VPD model checkpoint</a>.

The dataset contains RGB-D images from NYU depth v2. When using the ANYU dataset, this work should be cited along with our paper:
N. Silberman, D. Hoiem, P. Kohli, R. Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012.
